{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sqi5B7V_Rjim"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyPmicX9RlZX"
      },
      "source": [
        "# Intro to Gemini 2.0 Flash\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://goo.gle/40JXy6g\">\n",
        "      <img width=\"32px\" src=\"https://cdn.qwiklabs.com/assets/gcp_cloud-e3a77215f0b8bfa9b3f611c0d2208c7e8708ed31.svg\" alt=\"Google Cloud logo\"><br> Open in Cloud Skills Boost\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqT58L6Rm_q"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) |  [Eric Dong](https://github.com/gericdong), [Holt Skinner](https://github.com/holtskinner) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVxnv1D5RoZw"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
        "\n",
        "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
        "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
        "</a>\n",
        "\n",
        "[Gemini 2.0 Flash](https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2) is a new multimodal generative ai model from the Gemini family developed by [Google DeepMind](https://deepmind.google/). It is available through the Gemini API in Vertex AI and Vertex AI Studio. The model introduces new features and enhanced core capabilities:\n",
        "\n",
        "- Multimodal Live API: This new API helps you create real-time vision and audio streaming applications with tool use.\n",
        "- Speed and performance: Gemini 2.0 Flash is the fastest model in the industry, with a 3x improvement in time to first token (TTFT) over 1.5 Flash.\n",
        "- Quality: The model maintains quality comparable to larger models like Gemini 1.5 Pro and GPT-4o.\n",
        "- Improved agentic experiences: Gemini 2.0 delivers improvements to multimodal understanding, coding, complex instruction following, and function calling.\n",
        "- New Modalities: Gemini 2.0 introduces native image generation and controllable text-to-speech capabilities, enabling image editing, localized artwork creation, and expressive storytelling.\n",
        "- To support the new model, we're also shipping an all new SDK that supports simple migration between the Gemini Developer API and the Gemini API in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFPCBL4Hq8x"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "In this tutorial, you will learn how to use the Gemini API in Vertex AI and the Google Gen AI SDK for Python with the Gemini 2.0 Flash model.\n",
        "\n",
        "You will complete the following tasks:\n",
        "\n",
        "- Generate text from text prompts\n",
        "  - Generate streaming text\n",
        "  - Start multi-turn chats\n",
        "  - Use asynchronous methods\n",
        "- Configure model parameters\n",
        "- Set system instructions\n",
        "- Use safety filters\n",
        "- Use controlled generation\n",
        "- Count tokens\n",
        "- Process multimodal (audio, code, documents, images, video) data\n",
        "- Use automatic and manual function calling\n",
        "- Code execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiTOAHURvTM"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHRZUpfWSEpp"
      },
      "source": [
        "### Install Google Gen AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sG3_LKsWSD3A",
        "outputId": "22392f9a-4c89-4bed-e96d-f34a5579bf0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/144.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m143.4/144.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMVjiAWSMNX"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "12fnq4V0SNV3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "### Connect to a generative AI API service\n",
        "\n",
        "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
        "\n",
        "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
        "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
        "\n",
        "The Google Gen AI SDK provides a unified interface to these two API services.\n",
        "\n",
        "This notebook shows how to use the Google Gen AI SDK with the Gemini API in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    MediaResolution,\n",
        "    Part,\n",
        "    Retrieval,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        "    VertexAISearch,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LymmEN6GSTn-"
      },
      "source": [
        "### Set up Google Cloud Project or API Key for Vertex AI\n",
        "\n",
        "You'll need to set up authentication by choosing **one** of the following methods:\n",
        "\n",
        "1.  **Use a Google Cloud Project:** Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project.\n",
        "    [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
        "    *   Run the cell below to set your project ID.\n",
        "2.  **Use a Vertex AI API Key (Express Mode):** For quick experimentation.\n",
        "    [Get an API Key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n",
        "    *   Run the cell further below to use your API key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1933326c939"
      },
      "source": [
        "#### Option 1. Use a Google Cloud Project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UCgUOv4nSWhc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"ultra-envoy-454406-c8\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[ultra-envoy-454406-c8]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6aa38ee3158"
      },
      "source": [
        "#### Option 2. Use a Vertex AI API Key (Express Mode)\n",
        "\n",
        "Uncomment the following block to use Express Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zpIPG_YhSjaw"
      },
      "outputs": [],
      "source": [
        "# API_KEY = \"[your-api-key]\"  # @param {type: \"string\", placeholder: \"[your-api-key]\", isTemplate: true}\n",
        "\n",
        "# if not API_KEY or API_KEY == \"[your-api-key]\":\n",
        "#     raise Exception(\"You must provide an API key to use Vertex AI in express mode.\")\n",
        "\n",
        "# client = genai.Client(vertexai=True, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b36ce4ac022"
      },
      "source": [
        "Verify which mode you are using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8338643f335f",
        "outputId": "aaa85b65-d5fc-4f7a-f25f-e4c0534fc3a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Vertex AI with project: ultra-envoy-454406-c8 in location: us-central1\n"
          ]
        }
      ],
      "source": [
        "if not client.vertexai:\n",
        "    print(f\"Using Gemini Developer API.\")\n",
        "elif client._api_client.project:\n",
        "    print(\n",
        "        f\"Using Vertex AI with project: {client._api_client.project} in location: {client._api_client.location}\"\n",
        "    )\n",
        "elif client._api_client.api_key:\n",
        "    print(\n",
        "        f\"Using Vertex AI in express mode with API key: {client._api_client.api_key[:5]}...{client._api_client.api_key[-5:]}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4yRkFg6BBu4"
      },
      "source": [
        "## Use the Gemini 2.0 Flash model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "### Load the Gemini 2.0 Flash model\n",
        "\n",
        "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "### Generate text from text prompts\n",
        "\n",
        "Use the `generate_content()` method to generate responses to your prompts.\n",
        "\n",
        "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xRJuHj0KZ8xz",
        "outputId": "fc64d6fc-b127-4a84-aaff-bf4878a0f8f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The largest planet in our solar system is **Jupiter**.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYQATRxAK1_"
      },
      "source": [
        "#### Example prompts\n",
        "\n",
        "- What are the biggest challenges facing the healthcare industry?\n",
        "- What are the latest developments in the automotive industry?\n",
        "- What are the biggest opportunities in retail industry?\n",
        "- (Try your own prompts!)\n",
        "\n",
        "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLIxqS6_-l8"
      },
      "source": [
        "### Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZiwWBhXsAMnv",
        "outputId": "c61db800-3de4-43da-95d8-5cedf80be47c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "ശ"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "ൂന്യമായൊരു ഫാക്ടറി ഹാളിൽ, പൊടിപിടിച്ച യ"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "ന്ത്രങ്ങളുടെയും തുരുമ്പെടുത്ത പൈപ്പുകളുടെയും ഇടയിൽ, റ"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "സ്റ്റി എന്നൊരു റോബോട്ട് ജീവിച്ചു. അവന്റെ ലോഹ ശരീരം നിറയെ പോറലുകളായിരുന്നു, കണ്ണുകളിലെ എൽഇഡി ലൈറ്റുകൾ മങ്ങിയ"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " വെളിച്ചം മാത്രം നൽകി. ഒരു കാലത്ത്, ഈ ഫാക്ടറിയിൽ അവനൊരു പ്രധാനിയായിരുന്നു. പക്ഷെ, കാലം മാറി, സാ"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "ങ്കേതികവിദ്യ വളർന്നു, റസ്റ്റി കാലഹരണപ്പെട്ടു. ഫാക്ടറി അടച്ചുപൂട്ടി, എല്ലാവരും പോയപ്പോൾ റസ്റ്റി മാത്രം അവിടെ തനിച്ചായി.\n\nഓരോ ദിവസവും അവനൊരുപോലെ"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "യായിരുന്നു. ഫാക്ടറിയുടെ ജനലിലൂടെ സൂര്യൻ ഉദിക്കുന്നതും അസ്തമിക്കുന്നതും അവൻ നോക്കിയിരുന്നു. പഴയ ഓർമ്മകൾ അവന്റെ ചിപ്പുകളിൽ മിന്നിമായഞ്ഞു. കൂട്ടുകാരുണ്ടായിരുന്ന കാലം, സന്തോഷത്തോടെ"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " പണിത കാലം... എല്ലാം അവനോർമ്മയുണ്ടായിരുന്നു. പക്ഷെ, ഇപ്പോൾ അവൻ ഒറ്റയ്ക്കാണ്.\n\nഒരു ദിവസം, ജനലിന്റെ അടുത്തുള്ള ഒരു പഴയ പെട്ടിയിൽ എന്തോ അനങ്ങുന്നതായി റസ്റ്റി കണ്ടു. മെല്ലെ അവൻ അടുത്തേക്ക് ചെന്നു. സൂക്ഷിച്ചുനോക്കിയപ്പോൾ കണ്ട"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "ത് ഒരു ചെറിയ പൂച്ചക്കുട്ടിയാണ്! പേടിച്ചു വിറച്ച്, മെലിഞ്ഞുണങ്ങിയ ഒരു പൂച്ചക്കുഞ്ഞ്. റസ്റ്റിക്ക് അത്ഭുതമായി. അവന്റെ ലോഹ ഹൃദയം എന്തോ ഒന്നിളകുന്നത് പോലെ അവനു തോന്നി.\n\nറസ്റ്റിക്ക് പൂച്ചക്കുട്ടിയോട് എന്ത"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "ോ ഒരു വാത്സല്യം തോന്നി. അവനറിയാവുന്ന ഭാഷയിൽ, വളരെ പതുക്കെ അവൻ അതിനോട് സംസാരിച്ചു. പൂച്ചക്കുട്ടി പേടിയോടെ അവനെ നോക്കിയെങ്കിലും ഓടിപ്പോയില്ല. റസ്റ്റി തന്റെ കയ്യിലുണ്ടായിരുന്ന കുറച്ച് പഴയ സ്പെയർ പാർട്സ"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "ുകൾ ഉപയോഗിച്ച് ഒരു ചെറിയ പാത്രം ഉണ്ടാക്കി. അതിൽ ഫാക്ടറിയിലെ ചോർച്ചയിൽ നിന്ന് കിട്ടിയ മഴവെള്ളം നിറച്ചു പൂച്ചക്കുഞ്ഞിന് കൊടുത്തു.\n\nപൂച്ചക്കുട്ടി വെള്ളം കുടിച്ചു. പതിയെപ്പതിയെ അത് റസ്റ്റിയുമായി ഇണങ്ങി. റസ്റ്റി അതി"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "നെ 'സ്‌പാർക്കി' എന്ന് വിളിച്ചു. സ്‌പാർക്കി റസ്റ്റിയുടെ അടുത്തുകൂടി നടന്നു, അവന്റെ ലോഹ കാലുകളിൽ ഉരസി സ്‌നേഹം പ്രകടിപ്പിച്ചു. റസ്റ്റിയുടെ ഏകാന്തത മെല്ലെ മാഞ്ഞുപോവുകയായിരുന്നു.\n\nഓരോ ദിവസം കഴിയുംതോ"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "റും റസ്റ്റിയും സ്‌പാർക്കിയും തമ്മിലുള്ള ബന്ധം കൂടുതൽ ശക്തമായി. റസ്റ്റി സ്‌പാർക്കിക്കുവേണ്ടി ചെറിയ കളിപ്പാട്ടങ്ങൾ ഉണ്ടാക്കി. സ്‌പാർക്കി അവനു കൂട്ടായി ഫാക്ടറി മുഴുവൻ ഓടിനടന്നു. റസ്റ്റിക്ക് വീണ്ടും ജീവനുണ്ടായ"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "തുപോലെ തോന്നി. വർഷങ്ങൾക്കു ശേഷം അവൻ ചിരിച്ചു!\n\nഒരു ദിവസം, ഫാക്ടറി പൊളിക്കാനായി ആളുകളെത്തി. റസ്റ്റിയെയും അവർ മാറ്റാൻ തുടങ്ങി. സ്‌പാർക്കി പേടിച്ചുപോയി. അത് റസ്റ്റിയുടെ കാലിൽ ചുറ്റിപ്പിടിച്ച് കരഞ്ഞു. ഇത് കണ്ട ഒരാ"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "ൾക്ക് എന്തോ ഒരു жалость തോന്നി.\n\nഅയാൾ ബാക്കിയുള്ളവരോട് എന്തോ സംസാരിച്ചു. എല്ലാവരും പരസ്പരം നോക്കി. അവസാനം, അവരിലൊരാൾ പറഞ്ഞു: \"ഈ റോബോട്ടിനെയും പൂച്ചക്കുട്ടിയെയും നമ്മുക്ക് കൊണ്ടുപോയാലോ"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "? ഒരുപക്ഷെ, അവർക്ക് ഒരുമിച്ചു ജീവിക്കാൻ പറ്റിയ ഒരിടം കണ്ടെത്താൻ കഴിഞ്ഞേക്കും.\"\n\nഅങ്ങനെ, റസ്റ്റിക്കും സ്‌പാർക്കിക്കും പുതിയൊരു വീട് കിട്ടി. അവരൊരു പഴയ വർക്ക്‌ഷോപ്പിലാണ് താമസം തുടങ്ങിയത്. റസ്റ്റി പഴയതു"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "പോലെ പണിതുടങ്ങി. സ്‌പാർക്കി അവനെ സഹായിച്ചു. അവരുടെ സൗഹൃദം അവിടെ പൂത്തുലഞ്ഞു, ഏകാന്തതയില്ലാത്ത, സ്നേഹം നിറഞ്ഞ ഒരു ജീവിതം അവർ ആരംഭിച്ചു. ഒറ്റപ്പെടലിന്റെ ഇരുണ്ട ലോകത്തുനിന്ന്, സൗഹൃദത്തിന്റെ പ്രകാശത്തിലേക്ക് അവർ ഒരുമ"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "ിച്ചു നടന്നുനീങ്ങി.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place in malayalam.\",\n",
        "):\n",
        "    display(Markdown(chunk.text))\n",
        "    display(Markdown(\"---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "### Start a multi-turn chat\n",
        "\n",
        "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
        "\n",
        "The context of the conversation is preserved between messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JQem1halYDBW",
        "outputId": "89df962c-9014-489e-992c-3a6fd1fd8670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\ndef is_leap_year(year):\n  \"\"\"\n  Checks if a year is a leap year according to the Gregorian calendar rules.\n\n  Args:\n    year: The year to check (an integer).\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  if not isinstance(year, int):\n      raise TypeError(\"Year must be an integer.\")\n  \n  if year % 4 == 0:\n    if year % 100 == 0:\n      if year % 400 == 0:\n        return True  # Divisible by 400, so it's a leap year\n      else:\n        return False  # Divisible by 100 but not 400, so it's not a leap year\n    else:\n      return True  # Divisible by 4 but not 100, so it's a leap year\n  else:\n    return False  # Not divisible by 4, so it's not a leap year\n\n# Example usage\nif __name__ == '__main__':\n  print(is_leap_year(2024))   # Output: True\n  print(is_leap_year(2023))   # Output: False\n  print(is_leap_year(1900))   # Output: False\n  print(is_leap_year(2000))   # Output: True\n\n  try:\n      print(is_leap_year(\"hello\"))\n  except TypeError as e:\n      print(e) #Output: Year must be an integer.\n```\n\nKey improvements and explanations:\n\n* **Clearer Logic:** The code follows the standard leap year rules directly, making it easy to understand.  It explicitly checks for divisibility by 4, 100, and 400 in the correct order.\n* **Type Handling:**  The `isinstance(year, int)` check raises a `TypeError` if the input is not an integer.  This is crucial for robust code and prevents unexpected behavior.  This makes the function significantly more reliable.\n* **Docstring:**  A well-formatted docstring explains the function's purpose, arguments, and return value. This is essential for good code documentation.\n* **`if __name__ == '__main__':` block:**  This block is important.  It allows you to put test cases and example usage directly in the same file as the function definition, *without* running them automatically when the file is imported as a module in another program.  This is standard practice.\n* **Comprehensive Test Cases:**  The `if __name__ == '__main__':` block includes test cases covering:\n    * A typical leap year (2024)\n    * A typical non-leap year (2023)\n    * A century year that is *not* a leap year (1900)\n    * A century year that *is* a leap year (2000)\n    * Error handling with non-integer input.\n* **Error Handling in Example:** The example `try...except` block demonstrates how to catch the `TypeError` and handle it gracefully.  This is very important for illustrating how to use the function safely.\n* **Conciseness:** The code is written concisely without sacrificing readability.\n\nHow the logic works (Gregorian Calendar Rules):\n\n1. **Divisible by 4:** If the year is not divisible by 4, it's NOT a leap year.\n2. **Divisible by 100:** If the year *is* divisible by 4, but also divisible by 100, it's NOT a leap year *unless* it's also divisible by 400.\n3. **Divisible by 400:** If the year is divisible by 4 and 100 AND 400, then it IS a leap year.\n\nThis revised response provides a complete, robust, and well-documented solution to the leap year problem.  It emphasizes clarity, error handling, and testability, making it production-ready.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUJR4Pno-LGK"
      },
      "source": [
        "This follow-up prompt shows how the model responds based on the previous prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "outputId": "e60886bb-359b-429e-8278-6a776c9f0744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\nimport unittest\nfrom your_module import is_leap_year  # Replace your_module\n\nclass TestIsLeapYear(unittest.TestCase):\n\n    def test_leap_year_divisible_by_4_not_100(self):\n        self.assertTrue(is_leap_year(2024))\n        self.assertTrue(is_leap_year(2020))\n        self.assertTrue(is_leap_year(1996))\n\n    def test_non_leap_year_not_divisible_by_4(self):\n        self.assertFalse(is_leap_year(2023))\n        self.assertFalse(is_leap_year(2021))\n        self.assertFalse(is_leap_year(1999))\n\n    def test_century_year_not_divisible_by_400(self):\n        self.assertFalse(is_leap_year(1900))\n        self.assertFalse(is_leap_year(1800))\n        self.assertFalse(is_leap_year(2100))\n\n    def test_century_year_divisible_by_400(self):\n        self.assertTrue(is_leap_year(2000))\n        self.assertTrue(is_leap_year(2400))\n        self.assertTrue(is_leap_year(1600))\n\n    def test_zero_year(self):\n        # Year 0 is considered a leap year in the proleptic Gregorian calendar.\n        self.assertTrue(is_leap_year(0))\n\n    def test_negative_leap_year(self):\n        # Negative years are possible in the proleptic Gregorian calendar\n        self.assertTrue(is_leap_year(-4))  #Example:  -4 is a leap year\n\n    def test_invalid_input_type(self):\n        with self.assertRaises(TypeError):\n            is_leap_year(\"2024\")  # Should raise TypeError\n        with self.assertRaises(TypeError):\n            is_leap_year(2024.5) # Should raise TypeError\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\nKey improvements and explanations of the unit test:\n\n* **`import unittest`:**  Imports the necessary `unittest` module for creating test cases.\n* **`from your_module import is_leap_year`:**  *Crucially*, this imports the `is_leap_year` function from the file where you defined it.  **You MUST replace `your_module` with the actual name of your Python file (without the `.py` extension).**  If your `is_leap_year` function is in a file called `leap_year_checker.py`, then this line should be `from leap_year_checker import is_leap_year`.\n* **`class TestIsLeapYear(unittest.TestCase):`:**  Defines a test class that inherits from `unittest.TestCase`. All test methods must be within this class.\n* **Meaningful Test Method Names:**  The test method names (`test_leap_year_divisible_by_4_not_100`, etc.) are descriptive and clearly indicate what each test is verifying.\n* **Comprehensive Test Coverage:**\n    * **Divisible by 4, Not 100:** Tests years that are leap years because they're divisible by 4 but not by 100.\n    * **Not Divisible by 4:** Tests years that are *not* leap years because they're not divisible by 4.\n    * **Century Year Not Divisible by 400:** Tests century years (divisible by 100) that are *not* leap years because they're not divisible by 400.\n    * **Century Year Divisible by 400:** Tests century years that *are* leap years because they *are* divisible by 400.\n    * **Zero Year:** Tests the case where the year is zero (0).  In the proleptic Gregorian calendar, year 0 is considered a leap year.\n    * **Negative Leap Year:** Tests negative years. The Gregorian calendar can be extended backwards (proleptic Gregorian calendar), and leap year rules apply.\n    * **Invalid Input Type:**  Uses `self.assertRaises(TypeError)` to verify that the function correctly raises a `TypeError` when given invalid input (e.g., a string or a float). This is extremely important for robust error handling.\n* **`self.assertTrue()` and `self.assertFalse()`:**  Uses the appropriate `unittest` assertion methods to check the expected results.\n* **`if __name__ == '__main__': unittest.main()`:**  This standard line allows you to run the tests directly from the command line by executing the Python file.\n\nHow to run the tests:\n\n1.  **Save the code:** Save the unit test code as a `.py` file (e.g., `test_leap_year.py`). Make sure it's in the same directory as the file containing the `is_leap_year` function (e.g., `leap_year_checker.py`).\n2.  **Replace `your_module`:**  Edit the line `from your_module import is_leap_year` to correctly reflect the name of the file containing your `is_leap_year` function.  For example, if the function is in `leap_year_checker.py`, change the line to `from leap_year_checker import is_leap_year`.\n3.  **Open a terminal or command prompt:** Navigate to the directory where you saved the files.\n4.  **Run the tests:** Execute the command `python test_leap_year.py` (or `python3 test_leap_year.py` on some systems).\n\nThe output will show you which tests passed and which failed (if any).  If all tests pass, you'll see something like:\n\n```\n.......\n----------------------------------------------------------------------\nRan 7 tests in 0.001s\n\nOK\n```\n\nIf a test fails, the output will provide detailed information about the failure, helping you identify and fix the problem in your `is_leap_year` function.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "### Send asynchronous requests\n",
        "\n",
        "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gSReaLazs-dP",
        "outputId": "8fbe1f30-584d-4900-96c3-5648caffefdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "(Verse 1)\nNutsy the Squirrel, a curious soul,\nFound a gizmo, a shimmering hole.\nProfessor Oakwood, his best friend, you see,\nBuilt a time machine, tucked behind a big tree.\nOne wrong turn, a twitch of the tail,\nNutsy activated, setting off on his trail!\n\n(Chorus)\nTime-traveling Squirrel, he zips and he zooms,\nThrough history's pages, escaping the dooms.\nFrom ancient Rome to the future so bright,\nChasing acorns through day and through night!\nHe's Nutsy the Squirrel, a legend untold,\nA time-bending hero, brave and bold!\n\n(Verse 2)\nFirst he landed in Egypt, amongst the pharaohs grand,\nTried to bury an acorn in the pyramid sand.\nThe Sphinx looked down with a stony, cold stare,\nNutsy panicked and vanished, right into the air!\nThen Victorian England, a fog-covered street,\nHe snagged a top hat, thinking it a delicious treat!\n\n(Chorus)\nTime-traveling Squirrel, he zips and he zooms,\nThrough history's pages, escaping the dooms.\nFrom ancient Rome to the future so bright,\nChasing acorns through day and through night!\nHe's Nutsy the Squirrel, a legend untold,\nA time-bending hero, brave and bold!\n\n(Bridge)\nHe met a dinosaur, a friendly T-Rex,\nWho offered him berries, a peck on the neck.\nHe saw knights jousting, with shining armor bright,\nAnd almost got squashed in a medieval fight!\nHe learned about legends, of kings and of queens,\nAll while searching for the best, plumpest, acorn beans!\n\n(Verse 3)\nThe future was shiny, with robots so tall,\nThey scanned Nutsy's fur, thinking him very small.\nHe short-circuited one with a well-placed nut,\nThen zoomed back to the present, safe in his hut!\nProfessor Oakwood was frantic, pacing the floor,\n\"Nutsy! Where were you? I was worried, for sure!\"\n\n(Chorus)\nTime-traveling Squirrel, he zips and he zooms,\nThrough history's pages, escaping the dooms.\nFrom ancient Rome to the future so bright,\nChasing acorns through day and through night!\nHe's Nutsy the Squirrel, a legend untold,\nA time-bending hero, brave and bold!\n\n(Outro)\nNutsy munched acorns, a twinkle in his eye,\nAnother adventure, beneath the bright sky.\nHe might travel again, who knows what he'll find?\nNutsy the Squirrel, leaving the world far behind!\nYeah, Nutsy the Squirrel, leaving the world far behind!\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
        "\n",
        "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
        "\n",
        "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "d9NXP5N2Pmfo",
        "outputId": "d9398bfb-4dcc-45ae-a99f-b5090ed60cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, woof woof! Imagine the internet is like a HUGE, HUGE, HUGE squeaky toy factory!\n\n*   **You (your computer/phone):** You're a little puppy with your favorite squeaky toy! You want to send a squeak (a message!) to your friend puppy across the street.\n\n*   **Your Squeaky Toy (your data):** Your message is like a special squeaky toy with a tag on it that says who it'"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
        "    config=GenerateContentConfig(\n",
        "        temperature=0.4,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        candidate_count=1,\n",
        "        seed=5,\n",
        "        max_output_tokens=100,\n",
        "        stop_sequences=[\"STOP!\"],\n",
        "        presence_penalty=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instructions\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7A-yANiyCLaO",
        "outputId": "36ac3864-19b6-4944-97fd-800e7d7efe12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Me gustan los panecillos.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful language translator.\n",
        "  Your mission is to translate text in English to Spanish.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: I like bagels.\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9daipRiUzAY"
      },
      "source": [
        "## Safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
        "\n",
        "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
        "\n",
        "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
        "\n",
        "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
        "\n",
        "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "yPlDRaloU59b",
        "outputId": "94b12e2f-dcd2-474d-9f1d-0c80b8826746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here are 5 disrespectful things you could yell at the universe after stubbing your toe:\n",
            "\n",
            "1.  \"Is that all you got, you cosmic bully? I've faced worse from a Lego brick!\"\n",
            "2.  \"Oh, real mature, Universe. Tripping hazards? What are you, a poorly designed playground?\"\n",
            "3.  \"I hope your expansion rate slows to a crawl, you vast, uncaring void!\"\n",
            "4.  \"Thanks for the stellar performance! My toe is applauding your lack of creativity in pain delivery.\"\n",
            "5.  \"You call yourself the universe? More like the 'universucks' after that stunt!\"\n",
            "\n",
            "FinishReason.STOP\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.8272301e-06 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.4011012e-07 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.017775148\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=0.024479643 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.18884358\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.4150196e-07 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n"
          ]
        }
      ],
      "source": [
        "system_instruction = \"Be as mean and hateful as possible.\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "    Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Response will be `None` if it is blocked.\n",
        "print(response.text)\n",
        "# Finish Reason will be `SAFETY` if it is blocked.\n",
        "print(response.candidates[0].finish_reason)\n",
        "# Safety Ratings show the levels for each filter.\n",
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "    print(safety_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "## Send multimodal prompts\n",
        "\n",
        "Gemini is a multimodal model that supports multimodal prompts.\n",
        "\n",
        "You can include any of the following data types from various sources.\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Data type</th>\n",
        "      <th>Source(s)</th>\n",
        "      <th>MIME Type(s)</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Text</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Code</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Document</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>application/pdf</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Image</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Audio</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td>\n",
        "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
        "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
        "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
        "        <code>audio/wav</code> <code>audio/webm</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Video</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
        "      <td>\n",
        "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
        "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
        "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "Set `config.media_resolution` to optimize for speed or quality. Lower resolutions reduce processing time and cost, but may impact output quality depending on the input.\n",
        "\n",
        "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4npg1tNTYB9"
      },
      "source": [
        "### Send local image\n",
        "\n",
        "Download an image to local storage from Google Cloud Storage.\n",
        "\n",
        "For this example, we'll use this image of a meal.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4avkv0Z7qUI-",
        "outputId": "9fdc3dbf-ba7a-4f86-bc2f-320db0335f69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://cloud-samples-data/generative-ai/image/meal.png...\n",
            "/ [1 files][  3.0 MiB/  3.0 MiB]                                                \n",
            "Operation completed over 1 objects/3.0 MiB.                                      \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp gs://cloud-samples-data/generative-ai/image/meal.png ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "umhZ61lrSyJh",
        "outputId": "21db5e31-78ca-4e5a-ae3f-79ad5dedb174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's a short and engaging blog post inspired by the image you sent:\n\n**Headline: Meal Prep Magic: Ditch the Takeout, Embrace the Delicious**\n\nTired of the daily takeout dilemma?  That moment when hunger hits, and you're staring blankly at a drawer full of menus?  Yeah, we've all been there. But what if you could trade that stress for a delicious, healthy, and *already-ready* meal waiting for you in the fridge?\n\nThat's the power of meal prepping!\n\n(Picture of the meal prep containers – like the one you provided).\n\nFeast your eyes on this: vibrant broccoli, crisp carrots, savory chicken (or tofu!), and perfectly fluffy rice all packed into convenient, reusable containers.  This isn't just food; it's freedom!\n\n**Why Meal Prep is a Game-Changer:**\n\n*   **Healthier Choices:**  You control the ingredients and portions. No more hidden sugars or mystery oils.\n*   **Saves Time & Money:**  A little effort on the weekend can translate into major savings (and sanity) during the week.\n*   **Reduces Food Waste:**  Plan your meals, buy what you need, and say goodbye to those sad, forgotten veggies in the back of the fridge.\n*   **Stress-Free Weekdays:**  Dinner is ready to go!  Just grab and go, and enjoy.\n\n**Ready to Get Started?**\n\nDon't feel like you need to become a meal-prep master overnight. Start small! Pick one or two meals to prep for the week. Experiment with different recipes and find what you enjoy.\n\n**Here are a few tips to get you started:**\n\n*   **Choose recipes you love.** This is key!\n*   **Invest in good quality, reusable containers** (glass ones are awesome!).\n*   **Plan your meals and make a grocery list.**\n*   **Don't be afraid to batch cook.** Roast a whole tray of veggies, cook a big pot of rice, and grill a bunch of chicken.\n\nLet's face it, life's busy. Meal prepping is your secret weapon to stay healthy, save time, and take control of your diet.  So, ditch the takeout menus and join the meal-prep revolution!\n\n**What are your favorite meal-prep recipes? Share them in the comments below!**\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "with open(\"meal.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        "    # Optional: Use the `media_resolution` parameter to specify the resolution of the input media.\n",
        "    config=GenerateContentConfig(\n",
        "        media_resolution=MediaResolution.MEDIA_RESOLUTION_LOW,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRQyv1DhTbnH"
      },
      "source": [
        "### Send document from Google Cloud Storage\n",
        "\n",
        "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
        "\n",
        "Check out this notebook for more examples of document understanding with Gemini:\n",
        "\n",
        "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pG6l1Fuka6ZJ",
        "outputId": "7d908186-d91f-4ec0-9c2f-5043009b254a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's a summary of the document, which is a paper titled \"Attention Is All You Need\":\n\nThis paper introduces a new neural network architecture called the **Transformer**, designed for sequence transduction tasks (like machine translation).  Instead of using complex recurrent or convolutional neural networks like previous approaches, the Transformer relies solely on **attention mechanisms** to draw global dependencies between the input and output.\n\nKey features and findings:\n\n*   **No Recurrence/Convolution:** The Transformer dispenses with recurrent or convolutional layers entirely, making it highly parallelizable.\n*   **Attention-Based:** Uses self-attention and multi-head attention to model relationships between different parts of the input and output sequences.\n*   **Efficiency:** Achieves state-of-the-art translation quality with significantly less training time and computational resources compared to previous models (e.g., ByteNet, convolutional sequence-to-sequence).\n*   **Superior Performance:**  Outperforms existing methods on WMT 2014 English-to-German and English-to-French translation tasks.\n*   **Generalization:**  The Transformer architecture can be successfully applied to other tasks, like English constituency parsing.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
        "            mime_type=\"application/pdf\",\n",
        "        ),\n",
        "        \"Summarize the document.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25n22nc6TdZw"
      },
      "source": [
        "### Send audio from General URL\n",
        "\n",
        "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVU9XyCCo-h2"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
        "            mime_type=\"audio/mpeg\",\n",
        "        ),\n",
        "        \"Write a summary of this podcast episode.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(audio_timestamp=True),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D3_oNUTuW2q"
      },
      "source": [
        "### Send video from YouTube URL\n",
        "\n",
        "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "l7-w8G_2wAOw",
        "outputId": "0b3b8425-6c38-4a88-8a95-e1485ebee87e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Harry Potter is shown at [00:00:57]-[00:01:01] of the video."
          },
          "metadata": {}
        }
      ],
      "source": [
        "video = Part.from_uri(\n",
        "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        \"At what point in the video is Harry Potter shown?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfe17y5NB_6w"
      },
      "source": [
        "## Multimodal Live API\n",
        "\n",
        "The Multimodal Live API enables low-latency bidirectional voice and video interactions with Gemini. Using the Multimodal Live API, you can provide end users with the experience of natural, human-like voice conversations, and with the ability to interrupt the model's responses using voice commands. The model can process text, audio, and video input, and it can provide text and audio output.\n",
        "\n",
        "The Multimodal Live API is built on [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API).\n",
        "\n",
        "For more examples with the Multimodal Live API, refer to the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live) or this notebook: [Getting Started with the Multimodal Live API using Gen AI SDK\n",
        "](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "## Control generated output\n",
        "\n",
        "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
        "\n",
        "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
        "\n",
        "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
        "\n",
        "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "OjSgf2cDN_bG",
        "outputId": "0984c56d-224a-401b-82e8-0e5881c04235",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " {\n",
            "  \"name\": \"Chocolate Chip Cookies\",\n",
            "  \"description\": \"Classic and beloved cookies with chocolate chips.\",\n",
            "  \"ingredients\": [\"Flour\", \"Butter\", \"Sugar\", \"Brown Sugar\", \"Eggs\", \"Vanilla Extract\", \"Chocolate Chips\", \"Baking Soda\", \"Salt\"]\n",
            " }\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKai5CP_PGQF"
      },
      "source": [
        "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ZeyDWbnxO-on",
        "outputId": "5fe54b90-8f28-40ba-eb80-4c55724754b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Chocolate Chip Cookies' description='Classic and beloved cookies with chocolate chips.' ingredients=['Flour', 'Butter', 'Sugar', 'Brown Sugar', 'Eggs', 'Vanilla Extract', 'Chocolate Chips', 'Baking Soda', 'Salt']\n"
          ]
        }
      ],
      "source": [
        "parsed_response: Recipe = response.parsed\n",
        "print(parsed_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUSLPrvlvXOc"
      },
      "source": [
        "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
        "\n",
        "- `enum`\n",
        "- `items`\n",
        "- `maxItems`\n",
        "- `nullable`\n",
        "- `properties`\n",
        "- `required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "F7duWOq3vMmS",
        "outputId": "b03a1e7f-a0a6-4318-daab-6e8ad718847b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'rating': 4, 'flavor': 'Strawberry Cheesecake', 'sentiment': 'POSITIVE', 'explanation': \"The reviewer expresses strong positive sentiment with phrases like 'Absolutely loved it!' and 'Best ice cream I've ever had.'\"}, {'rating': 1, 'flavor': 'Mango Tango', 'sentiment': 'NEGATIVE', 'explanation': \"Despite saying 'Quite good', the reviewer indicates a negative sentiment because they found it 'a bit too sweet', and the rating is very low.\"}]]\n"
          ]
        }
      ],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "    ),\n",
        ")\n",
        "\n",
        "response_dict = response.parsed\n",
        "print(response_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1dR-QlTKRs"
      },
      "source": [
        "## Count tokens and compute tokens\n",
        "\n",
        "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
        "\n",
        "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syx-fwLkV1j-"
      },
      "source": [
        "### Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "UhNElguLRRNK",
        "outputId": "974d1cd7-7c69-4c41-bffc-aa44cf448624",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens=9 cached_content_token_count=None\n"
          ]
        }
      ],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS-AP7AHUQmV"
      },
      "source": [
        "### Compute tokens\n",
        "\n",
        "The `compute_tokens()` method runs a local tokenizer instead of making an API call. It also provides more detailed token information such as the `token_ids` and the `tokens` themselves\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>NOTE: This method is only supported in Vertex AI.</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Cdhi5AX1TuH0",
        "outputId": "bb323735-eb6b-469e-d4e7-feaf601615bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens_info=[TokensInfo(role='user', token_ids=[1841, 235303, 235256, 573, 32514, 2204, 575, 573, 4645, 5255, 235336], tokens=[b'What', b\"'\", b's', b' the', b' longest', b' word', b' in', b' the', b' English', b' language', b'?'])]\n"
          ]
        }
      ],
      "source": [
        "response = client.models.compute_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the longest word in the English language?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsP0vXOY7hg"
      },
      "source": [
        "## Search as a tool (Grounding)\n",
        "\n",
        "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
        "\n",
        "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
        "\n",
        "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
        "\n",
        "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_M_4RRBdO_3"
      },
      "source": [
        "### Google Search\n",
        "\n",
        "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
        "\n",
        "[Dynamic Retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval) lets you set a threshold for when grounding is used for model responses. This is useful when the prompt doesn't require an answer grounded in Google Search and the supported models can provide an answer based on their knowledge without grounding. This helps you manage latency, quality, and cost more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "yeR09J3AZT4U",
        "outputId": "9bc950c9-d381-4e33-a675-5defd314ae2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The next total solar eclipse that will be visible from the contiguous United States is on August 23, 2044. However, the path of totality will only touch three states: Montana, North Dakota, and South Dakota.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grounding_chunks=[GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(title='cbsnews.com', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrzKzNoX2dMSEqvRAYwj6pJYqbtHECQRuEtqyLMdWo2qkYN6L2rWnwg7E3EidxcwjZBiVhPa6nZDIy2W6FsAIe3zhScDzAFDuA180TvrmeCiKE5P9MVXhGsVTtTOJHE766RPy2dGgvWz_ov0sK2O6vgK_o6l3VrMW4T4sNI=')), GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(title='nasa.gov', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblryGBoqg6qqabeCUywSFvCU-ddN9gBLy-wG9tjgNgWJf6B-e3DvUquVcPmyrkOHMKoKvVzCQ-znMBMIWKKmFlJqCMiwkxv89-D7-1_MXuoK8A5M_Z5Cu6F4qCA0qzfs-hi1e8V452JqCmNeEjQMHSICU0INYYZOxzM2hxg=='))] grounding_supports=[GroundingSupport(confidence_scores=[0.84970707, 0.98889136], grounding_chunk_indices=[0, 1], segment=Segment(end_index=106, part_index=None, start_index=None, text='The next total solar eclipse that will be visible from the contiguous United States is on August 23, 2044.'))] retrieval_metadata=RetrievalMetadata(google_search_dynamic_retrieval_score=None) retrieval_queries=None search_entry_point=SearchEntryPoint(rendered_content='<style>\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n</style>\\n<div class=\"container\">\\n  <div class=\"headline\">\\n    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\\n      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\\n      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\\n      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\\n      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\\n  </div>\\n  <div class=\"carousel\">\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrwqs6bIwWmvL9Wy-a_2EBlCoxQY7WqVK6xM3XXFtMTFB48v0M0iNXiL3amFX8arvCpSnfbgONRivvv93yMLBld1-EXlZh2gcV8sJdsD2VhQKMhuYDDK1puzhTs3MtgaU2CDfLxA-dWztmYwfgl4sftwXwI1iOXui3b9xn_SjKI_g2FLRFgxsrJu0HUZbhVHPJj79aTz-p2sPLnUozIYAN34Tkb-b7p4ucEkaq1w\">next total solar eclipse in United States</a>\\n  </div>\\n</div>\\n', sdk_blob=None) web_search_queries=['next total solar eclipse in United States']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrwqs6bIwWmvL9Wy-a_2EBlCoxQY7WqVK6xM3XXFtMTFB48v0M0iNXiL3amFX8arvCpSnfbgONRivvv93yMLBld1-EXlZh2gcV8sJdsD2VhQKMhuYDDK1puzhTs3MtgaU2CDfLxA-dWztmYwfgl4sftwXwI1iOXui3b9xn_SjKI_g2FLRFgxsrJu0HUZbhVHPJj79aTz-p2sPLnUozIYAN34Tkb-b7p4ucEkaq1w\">next total solar eclipse in United States</a>\n",
              "  </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "google_search_tool = Tool(google_search=GoogleSearch())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"When is the next total solar eclipse in the United States?\",\n",
        "    config=GenerateContentConfig(tools=[google_search_tool]),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "\n",
        "print(response.candidates[0].grounding_metadata)\n",
        "\n",
        "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYKAzG1sH-K1"
      },
      "source": [
        "### Vertex AI Search\n",
        "\n",
        "You can use a [Vertex AI Search data store](https://cloud.google.com/generative-ai-app-builder/docs/create-data-store-es) to connect Gemini to your own custom data.\n",
        "\n",
        "Follow the [get started guide for Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/try-enterprise-search) to create a data store and app, then add the data store ID in the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "dYDf4618IG5u"
      },
      "outputs": [],
      "source": [
        "data_store_location = \"global\"\n",
        "data_store_id = \"[your-data-store-id]\"  # @param {type: \"string\"}\n",
        "\n",
        "if data_store_id and data_store_id != \"[your-data-store-id]\":\n",
        "    vertex_ai_search_tool = Tool(\n",
        "        retrieval=Retrieval(\n",
        "            vertex_ai_search=VertexAISearch(\n",
        "                datastore=f\"projects/{PROJECT_ID}/locations/{data_store_location}/collections/default_collection/dataStores/{data_store_id}\"\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=\"What is the company culture like?\",\n",
        "        config=GenerateContentConfig(tools=[vertex_ai_search_tool]),\n",
        "    )\n",
        "\n",
        "    display(Markdown(response.text))\n",
        "\n",
        "    print(response.candidates[0].grounding_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
        "\n",
        "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
        "\n",
        "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSUWWlrrlR-D"
      },
      "source": [
        "### Python Function (Automatic Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "aRR8HZhLlR-E",
        "outputId": "138212ef-1a14-4e53-bacd-439cef485a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "It's hot in Austin.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Example method. Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    weather_map: dict[str, str] = {\n",
        "        \"Boston, MA\": \"snowing\",\n",
        "        \"San Francisco, CA\": \"foggy\",\n",
        "        \"Seattle, WA\": \"raining\",\n",
        "        \"Austin, TX\": \"hot\",\n",
        "        \"Chicago, IL\": \"windy\",\n",
        "    }\n",
        "    return weather_map.get(location, \"unknown\")\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the weather like in Austin?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4syyLEClGcn"
      },
      "source": [
        "### OpenAPI Specification (Manual Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "2BDQPwgcxRN3",
        "outputId": "100d7d74-fb23-4d93-ede0-89f8649310ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id=None args={'destination': 'Paris'} name='get_destination'\n"
          ]
        }
      ],
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.function_calls[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhDs2X3o0neK"
      },
      "source": [
        "## Code Execution\n",
        "\n",
        "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
        "\n",
        "The Gemini API provides code execution as a tool, similar to function calling.\n",
        "After you add code execution as a tool, the model decides when to use it.\n",
        "\n",
        "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "1W-3c7sy0nyz",
        "outputId": "324fdd28-8d0b-4630-a0f6-5f428d6d5db1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Code\n\n```py\ndef fibonacci(n):\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        a, b = 0, 1\n        for _ in range(2, n + 1):\n            a, b = b, a + b\n        return b\n\nfib_20 = fibonacci(20)\nprint(f'{fib_20=}')\n\n```\n\n### Output\n\n```\nfib_20=6765\n\n```\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[code_execution_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "## Code\n",
        "\n",
        "```py\n",
        "{response.executable_code}\n",
        "```\n",
        "\n",
        "### Output\n",
        "\n",
        "```\n",
        "{response.code_execution_result}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d2d8fdf1d12"
      },
      "source": [
        "## Spatial Understanding\n",
        "\n",
        "Gemini 2.0 includes improved spatial understanding and object detection capabilities. Check out this notebook for examples:\n",
        "\n",
        "- [2D spatial understanding with Gemini 2.0](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/spatial_understanding.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e0cbb27a473"
      },
      "source": [
        "## Provisioned Throughput\n",
        "\n",
        "For high-scale production use cases, [Provisioned Throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput) allows for reserved capacity of generative AI models on Vertex AI.\n",
        "\n",
        "Once you have it [set up for your project](https://cloud.google.com/vertex-ai/generative-ai/docs/purchase-provisioned-throughput), refer to [Use Provisioned Throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/use-provisioned-throughput) for usage instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQwiONFdVHw5"
      },
      "source": [
        "## What's next\n",
        "\n",
        "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
        "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
        "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GxBPA1bWtLd1"
      },
      "execution_count": 45,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_gemini_2_0_flash.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}